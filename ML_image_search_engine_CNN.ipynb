{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Search Engine with CNN using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n",
      "Is using GPU? False\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D,BatchNormalization\n",
    "from keras.layers import MaxPooling2D,Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "import cv2\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import random\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.mkdir('models')\n",
    "    \n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Is using GPU?', False if tf.config.list_physical_devices('GPU') == [] else True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "# print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        assert os.path.exists(self.data_path), 'Insert a valid path!'\n",
    "\n",
    "        # get class list\n",
    "        self.data_classes = os.listdir(self.data_path)\n",
    "\n",
    "        # init mapping dict\n",
    "        self.data_mapping = {}\n",
    "\n",
    "        # populate mapping dict\n",
    "        for c, c_name in enumerate(self.data_classes):\n",
    "            temp_path = os.path.join(self.data_path, c_name)\n",
    "            temp_images = os.listdir(temp_path)\n",
    "\n",
    "            for i in temp_images:\n",
    "                img_tmp = os.path.join(temp_path, i)\n",
    "\n",
    "                if img_tmp.lower().endswith(('.jpg', '.jpeg')):\n",
    "                    if c_name == 'distractor':\n",
    "                        self.data_mapping[img_tmp] = -1\n",
    "                    else:\n",
    "                        self.data_mapping[img_tmp] = c_name\n",
    "\n",
    "        print('Loaded {:d} from {:s} images'.format(len(self.data_mapping.keys()),\n",
    "                                                    self.data_path))\n",
    "\n",
    "    def get_data_paths(self):\n",
    "        # returns a list of imgpaths and related classes\n",
    "        images = []\n",
    "        classes = []\n",
    "        for img_path in self.data_mapping.keys():\n",
    "            if img_path.lower().endswith(('.jpg', '.jpeg')):\n",
    "                images.append(img_path)\n",
    "                classes.append(self.data_mapping[img_path])\n",
    "        return images, np.array(classes)\n",
    "\n",
    "\n",
    "    def num_classes(self):\n",
    "        # returns number of classes of the dataset\n",
    "        return len(self.data_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get data_path\n",
    "# data_path = '/content/drive/MyDrive/ML/dataset'\n",
    "data_path = 'dataset_rahin'\n",
    "# we define training_path\n",
    "training_path = os.path.join(data_path, 'training')\n",
    "\n",
    "# we define validation path, query and gallery\n",
    "validation_path = os.path.join(data_path, 'validation')\n",
    "gallery_path = os.path.join(validation_path, 'gallery')\n",
    "query_path = os.path.join(validation_path, 'query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 47986 from dataset_rahin\\training images\n",
      "Loaded 295 from dataset_rahin\\validation\\gallery images\n",
      "Loaded 26 from dataset_rahin\\validation\\query images\n"
     ]
    }
   ],
   "source": [
    "training_dataset = Dataset(data_path=training_path)\n",
    "gallery_dataset = Dataset(data_path=gallery_path)\n",
    "query_dataset = Dataset(data_path=query_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Importing the data folder and giving a shuffle\"\"\"\n",
    "dataset=[]\n",
    "labels=[]\n",
    "random.seed(420)\n",
    "imagePaths = sorted(list(os.listdir(\"dataset_rahin/training\")))\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images in imagePaths:\n",
    "    path=sorted(list(os.listdir(\"dataset_rahin/training/\"+images)))\n",
    "    for i in path:\n",
    "        try:\n",
    "            image = cv2.imread(\"dataset_rahin/training/\"+images+'/'+i) #using opencv to read image\n",
    "#             print(images, \": \", i, \": \", image.shape)\n",
    "#             image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (128,128)) \n",
    "#             image = img_to_array(image) #converting image info to array\n",
    "            dataset.append(image)\n",
    "\n",
    "            l = label = images\n",
    "            labels.append(l)\n",
    "    #         print(l)\n",
    "        except:\n",
    "            print(\"Error with loading and resizing image\", \" \", images, \": \", i)\n",
    "            #code to move to next frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converting to numpay array\"\"\"\n",
    "dataset = np.array(dataset, dtype=\"float32\") / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "\"\"\"Here we are using LabelBinarizer to scale data because it does not need data in integer encoded form first to convert into its respective encoding\"\"\"\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splitting dataset into train and test\"\"\"\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(dataset,labels,test_size=0.3,random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# y_train2 = tf.keras.utils.to_categorical(np.asarray(y_train.factorize()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_three_classes(x, y):\n",
    "#     indices_0, _ = np.where(y == 0.)\n",
    "#     indices_1, _ = np.where(y == 1.)\n",
    "#     indices_2, _ = np.where(y == 2.)\n",
    "\n",
    "#     indices = np.concatenate([indices_0, indices_1, indices_2], axis=0)\n",
    "    \n",
    "#     x = x[indices]\n",
    "#     y = y[indices]\n",
    "    \n",
    "#     count = x.shape[0]\n",
    "#     indices = np.random.choice(range(count), count, replace=False)\n",
    "    \n",
    "#     x = x[indices]\n",
    "#     y = y[indices]\n",
    "    \n",
    "#     y = tf.keras.utils.to_categorical(y)\n",
    "    \n",
    "#     return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"Train whole set:\")\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(\"Test whole set:\")\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# x_train2, y_train2 = get_three_classes(x_train, y_train)\n",
    "# x_test2, y_test2 = get_three_classes(x_test, y_test)\n",
    "# print(\"Train subset:\")\n",
    "# print(x_train2.shape, y_train2.shape)\n",
    "# print(\"Test subset:\")\n",
    "# print(x_test2.shape, y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Visualize Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['aeroplane', 'car', 'bird']\n",
    "\n",
    "def show_random_examples(x, y, p):\n",
    "    indices = np.random.choice(range(x.shape[0]), 10, replace=False)\n",
    "    \n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    p = p[indices]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2, 5, 1 + i)\n",
    "        plt.imshow(x[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        col = 'green' if np.argmax(y[i]) == np.argmax(p[i]) else 'red'\n",
    "        plt.xlabel(np.argmax(p[i]), color=col)\n",
    "    plt.show()\n",
    "    \n",
    "show_random_examples(x_train, y_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_examples(x_test, y_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Input, Dense\n",
    "\n",
    "def create_model():\n",
    "    def add_conv_block(model, num_filters):\n",
    "        model.add(Conv2D(num_filters, 3, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(num_filters, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "#         model.add(Dropout(0.5))\n",
    "        return model\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Input(shape=(128, 128,3)))\n",
    "    \n",
    "    model = add_conv_block(model, 32)\n",
    "    model = add_conv_block(model, 64)\n",
    "    model = add_conv_block(model, 128)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(98, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam', metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit(\n",
    "    x_train/255., y_train,\n",
    "    validation_data=(x_test/255., y_test),\n",
    "    epochs=10, batch_size=128,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'models/model_{val_accuracy:.3f}.h5',\n",
    "            save_best_only=True, save_weights_only=False,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = h.history['accuracy']\n",
    "val_accs = h.history['val_accuracy']\n",
    "\n",
    "plt.plot(range(len(accs)), accs, label='Training')\n",
    "plt.plot(range(len(accs)), val_accs, label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= tf.keras.models.load_model('models/model_0.628.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_examples(x_test, y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
